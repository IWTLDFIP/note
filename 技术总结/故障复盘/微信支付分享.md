丨 导语 整理微信支付2020-2024近5年故障单，抽象总结共性踩坑点，挖掘导致故障背后的主要原因，提升系统设计能力。技术交流，不涉及具体故障细节。

微信支付一直以来比较重视故障的复盘，对每次故障都做了详细记录，记录故障的影响范围、故障恢复时长、故障恢复的操作步骤、故障应对措施，挖掘可以改进优化的地方，积累了大量故障的素材。这些故障也不都是造成多大影响的事故，可能是引发了商户或者少量用户的客诉的一些问题，或者一些体验上需要提升优化。相信这些踩过的坑对微信支付整体可用性的持续提升也起到了推动作用，相同的研发环境下，类似的问题大概率我们也都会遇到，总结这些故障发生的原因，对日常系统设计能力的提高也有很大的帮助。

# **故障原因分布**

首先对所有引起故障的原因做一个简单的归类，以下是不同原因导致故障的排序。有些故障是多个因素导致的，这里按个人理解的最主要的导致因素整理：

![[Pasted image 20250110151350.png]]

# **程序缺陷**

### **缺陷模式：程序缺陷**

这些故障中，由程序缺陷引起的故障次数最多。程序缺陷的原因有很多，无法一一列举，大致有以下几类：

1. 改动现有的代码逻辑所涉及的影响面评估不足，只关注了局部的逻辑，存在逻辑盲区，导致灰度或者发布后出现问题
2. 重构现有的逻辑，引入了bug
3. 和历史债务相关，由于历史债务或者设计不合理，触发了隐藏的bug
4. 修复历史bug时引入了新的bug
5. 版本升级的兼容性问题，包括客户端新旧版本的兼容性问题、新旧逻辑的兼容性问题，历史数据的兼容性问题等
6. 测试代码误带上现网，CodeReview 时没能发现
7. 重构或产品特性有较大改动，导致测试Case不够，存在逻辑问题，引发故障
8. 需求频繁改动，周期拉得又比较长，没能有效沟通对齐，遗漏一些实现的逻辑
9. 代码实现问题，导致机器资源使用问题：如CPU使用率上涨，缓慢的内存泄漏导致OOM，某些异常情况下出现空指针导致coredump、句柄泄露等
10. 文件执行权限变动导致无法执行（chmod 文件权限，这类一般都是新创建文件时忘记修改文件可执行属性导致）
11. 字段类型变更导致数据解析失败，未遵守约定实现
12. 业务逻辑中存在hardcode特殊逻辑，如hardcode特殊商户、用户、日期等，引发意外
13. 代码耦合严重，修改一个地方不小心影响到其他的地方
14. 系统实现不够内聚，导致霰弹式修改，遗漏了一些改动
15. 边界情况考虑不周
16. 存在特殊逻辑，对其逻辑兼容性不够
17. 存在代码未合并进主干，遗漏发布
18. 代码缺陷产生了脏数据，出现了横向影响
19. 对使用的接口的内部实现缺少深入了解，极端情况可能出现未知的后果
20. 等等...

**一些优化改进措施：**

1. 补充测试用例
2. 代码 CR更仔细
3. 加强测试用例覆盖
4. 故障发现时长，缺少监控告警措施
5.  缺少应对故障的快速应对措施（SOP）
6. 健康度体检
7. 等等...

# **依赖方故障**

系统很少是孤立存在的，依赖外部服务或被外部服务依赖就难免受上下游服务的影响，依赖方故障是容易引发故障的主要原因之一。

### **缺陷模式 ：依赖故障**

这类故障银行侧或者商户侧异常居多，如银行系统异常，导致资金入账处理不及时。识别依赖方异常后，走异常分支处理逻辑。需要外部依赖时，首先分析一下依赖是否强依赖，是否可以通过一些流程或优化手段去掉这个依赖，能减少一个依赖就没必要额外增加一个依赖风险。很多故障事后复盘发现，之前的依赖是不合理的，甚至强依赖的也可以通过一些手段优化后变成弱依赖。如果不得不依赖，那这个依赖是否是关键路径，如果非关键路径，就不要当做关键链路，异常时考虑忽略异常或降级，不中断服务。如果是关键路径，那是否是强同步依赖，故障时能否可以先受理下来，如代扣充值，降级到异步处理，依赖服务恢复后进行补偿处理。如果是关键路径，且必须同步实时提供服务，挂了就不能提供服务，有条件能否找到对应备份服务提供方？比如备份的服务商，故障时主动或自动熔断切换到备份服务。如果只有一份，且挂了也不能降级，独一份，不得不接受当前故障，这个时候可以给用户更友好的提示或挂系统维护公告，暂停服务给用户明确的预期，避免引起用户恐慌。“独一份”的服务，经常会遇到关键依赖方技术能力较弱的情况，系统稳定性不够或扛不住大规模的并发，需要多为他们考虑一些容灾措施，比如容量问题，上游是否可以对下游增加一些限频？有没有过载保护措施？能否依据系统的实时负载能力，有多大处理能力就发多少请求？让依赖方在其能力范围内提供持续的服务。

![](https://km.woa.com/asset/000100022412009fa565a95a804ffe02?height=924&width=1322)以上是在上游在依赖方故障时可以明确感知的情况，有明确失败的场景就有措施可以应对。还有一种是比较麻烦的，就是依赖方接口告诉你处理成功了，但是实际未执行到位，比如销账充值，告知充值成功，但实际未到账，这种表面的成功是很难发现和应对的，需要通过一些提示给用户一些预期，比如充值可能会有延时，尽量减少用户疑惑。

  

# **容灾缺陷**

### **缺陷模式 ：单点问题**

这里的单点不是分布式系统设计中主从节点主节点单点故障。这里的单点包括存储单点、服务单点、部署单点等。如核心的存储只有一组DB单点；核心的服务只有一套，故障时影响全部。关键服务最好进行条带化拆分，不同 set 之间资源隔离，即使出问题也能极大降低影响面。

![](https://km.woa.com/asset/00010002241200724dac7de89d4ec002?height=486&width=1000)

### **缺陷模式 ：无效的访问**

系统设计实现时未及时关注到隐藏的一些依赖或者调用，其实是无效的。比如设计不合理，调用了一个宽接口，实际只需要其中一个功能子集。尤其是在项目时间比较紧张的情况下，不小心容易带上一些无效的请求，如曾经访问存储可能会无意间带上Count 总数的查询，可能无法命中索引，导致扫全表。请求量级不大或数据量不大时问题不大，问题容易被掩盖，随着业务的增长到达瓶颈，导致突发失败。确认调用都是必须要做和实现的，多一个调用多一份依赖就多一份风险。

### **缺陷模式 ：慢查询+重试放大效应，拖垮服务**

慢查询的原因主要的 2 类：

1. 比如查询粒度不合理，一次性查询过多的数据，导致超时，用户重试又放大了请求次数，积压资源占用，过了临界点影响其他正常请求，进一步导致用户更多地重试，恶性循环下去形成雪崩。
2. 查询没有走到索引，或扫全表。业务数据不多或者业务量级不大时，潜在的风险没有暴露出来，随着量级的增长和数据的累积，过了某个资源临界点引发服务整体过载

### **缺陷模式：主备不一致问题**

特别指的是DB 的主备延迟：

1. DB变更、DDL操作等，导致主备有延迟，而业务使用过程中走的备机的查询，导致数据状态不一致引发异常
2. Master 重启在某些极端异常情况下可能会导致DB的半同步被降级为异步，且Master依然会接受请求，备机同步不到数据，主备切换后，导致新老master数据不一致的情况

### **缺陷模式：链接拉爆**

DB的读写超时时间设置不合理，上游模块超时时间设置的比较短，DB异常时上游超时后重试，DB连接又还没有断开，上游失败导致框架重试，或者用户的重试，请求爆炸，最终导致链接拉爆，DB被拖死

**![](https://km.woa.com/asset/000100022412000f1d7e25b97f4b9f02?height=422&width=872)**

### **缺陷模式：接口超时时间不合理**

接上一条爆链的问题，类似地，在业务存在上下游模块的调用关系时，不同接口的超时时间不同，下游的接口超时时间比上游调用的超时时间长，上游超时后用户重试或框架重试放大，同样会存在级联拉爆底层服务的风险。

![](https://km.woa.com/asset/0001000224120061f2dd60d781428f02?height=482&width=648)

### **缺陷模式：Cache的隐患**

1. 单机故障或更新机制失效，导致cache更新失败，上游当做了关键路径，短时间又难以数据清理和恢复
2. cache被击穿，如缺乏缓存预热，导致压力传递给后端，导致后端过载。使用缓存常的关注三点：缓存雪崩、缓存击穿、缓存穿透，其原因和解决办法都有详细的介绍可查，不再赘述

### **缺陷模式：读写分离使用不当**

主要是数据库使用场景下，为了容灾，DB采用主备模式，一主拖N备，按业务场景需要主备之间同步采用一种同步策略，如半同步模式，有一台备机同步成功，master就认为主备同步成功。从性能角度考虑，业务一般采用主写备读的策略，即写或更新通过 master 来操作，查询通过备机进行查询，如更新或者查询数据的状态。正常情况下，主备的延迟是很低的，这种模式运行OK。一旦出现异常，导致主备有延迟，备机和主机的数据就会出现不一致的情况，如果业务强依赖这种数据一致性就会导致处理流程异常，具体情况需要业务自己评估。对一致性要求是关键依赖可以读取 Master，避免这种不一致的问题。有些场景不强依赖一致性的问题，如某些异步任务，可以不拉master，从某个slave拉取，避免大而重的非关键任务影响现网服务。

### 缺陷模式：熔断缺失

下游服务持续性故障，短时间无法恢复，持续影响用户。这类可以考虑增加熔断机制，尤其缴费类业务，下游服务失败率过高容易引起投诉，主动熔断提升用户体验。

![](https://km.woa.com/asset/00010002241200ebb3aa0010f4484602?height=532&width=492)

### **缺陷模式：没有生效的兜底逻辑**

高可用系统设计的一种策略就是出现异常时系统是否可降级。关键流程或者关键数据处理，当主逻辑出现异常时一般会设计一个兜底流程或者补偿措施，但这里往往容易忽略一些问题：

1. 兜底逻辑一定能生效吗？兜底逻辑往往是一个备份的逻辑，正常情况下很少有机会执行到，其正确性容易被忽略，或者随着时间的推移，兜底逻辑已经不能适应当前的整体业务了。极端情况下，发现兜底逻辑兜不住了。
2. 假设兜底作为备份的处理链路是生效的，另外一个角度，备份链路是否有足够的资源应对突发的异常。一般兜底都会认为是小概率事件或者数量量不大。兜底逻辑没有故障时可能永远都不会走到，往往分配的资源也是比较少的，一但发生了，兜底逻辑是否有足够的容量抗下里这些请求需要自己判断和评估，这里有可能需要备份的资源，在降本增效的背景下需要取舍考虑。
3. 兜底生效了，但兜底逻辑也可能会执行失败，如果兜底也最终执行失败了，是否有应对措施？是否需要继续套娃兜底还是可最终人工介入，具体情况具体分析，一般都需要监控上报，能及时感知到这种情况。

### **缺陷模式：组件使用不当**

组件的使用姿势不正确导致故障。如cl5，事后结果需要业务上报，如果未上报，当机器或者服务出现异常时也不能够被及时剔除。

### **缺陷模式：服务重启时屏蔽的风险**

背景是，后台服务模块重启瞬间大概率会出现系统失败，为了避免重启造成服务抖动，一般模块发布都会打开服务变更前屏蔽，重启前把流量切给其他对等的机器，重启完成后再切回流量恢复。重启期间屏蔽的流量分摊到其他机器时，可能会导致其他机器过载，有当机器数比较少或者发布比较快速时候，更容易引发问题。如3园区 3 台机器，变更前屏蔽，流量会分到其他2个园区， 但此时2个园区否扛住？简单换算一下，N 台机器，重启其中 1 台，1/N 的流量分布给其他 N-1 台，那每台机器的负载变成：1/N+1/N*(N-1) ，这个负载必须要小于单机负载的最高水位M：1/N+1/N*(N-1) <M；具体的负载瓶颈可以依据服务消耗型的资源情况进行评估，如CPU、内存等。

### **缺陷模式：无效的重试**

前面已经描述过重试可能带来的风险，具体来看，重试来源主要有两类：模块的框架重试以及因请求失败导致的用户重试。尤其是系统有很多模块的情况下，有很多层级调用关系。为了提高可用性，不同层之间调用往往会设置调用自动重试。假设每一层重试 M 次，有 N 层，最严重的情况下，最低层的模块会导致 M^(N-1)次的请求，这种指数级的放大是非常恐怖的量级。有时为了容灾，写代码时又自己进行了重试。重试动作应该尽量往上迁移，避免对下游过大的放大压力。另外一个维度，来源是用户的重试。如果系统真的挂了，用户大量的无效重试也是没有意义的，针对 Client 端也可以做一些容灾措施。如果这个产品有Client端，那么可以在 Client 端简单做一个熔断，很多时候 web 接入的时候接口会对用户维度做频率限制，但往往这个限制是比较宽松的，Client 端连续识别到接口失败比例很高后，可以考虑自动屏蔽入口，请用户暂停操作，避免用户大量无效的重试，对后端的恢复增加一些机会。

# **变更管理**

前面程序缺陷大都也是通过变更发布引起的，这里的变更管理主要是非程序缺陷之外的变更因素。

### **缺陷模式：发布顺序问题**

1. 版本发布需要多个步骤多次变更操作，事前发布未整理发布checklist，发布顺序不合理，导致发布问题
2. 框架迁移，旧的框架迁移到新框架，新架构的发布操作方式和潜在的风险点没有评估到位

**缺陷模式：缩容&裁撤的风险**

1. 未考虑 byset 条带化部署情况，缩容集中缩掉掉了某些set 的机器，导致被缩掉机器set资源不足服务过载
2. 裁撤缩掉了还有流量的机器，机器上可能存在周期性的任务或商户周期性的调用
3. 缩容粒度过大，未遵循灰度逐步缩容，一次性缩容过大导致容量不足

### **缺陷模式：变更夹带**

变更发布夹带了其他的特性，未确认变更的 differ，夹带了不应该发布的功能，而又未及时关注发布后的影响

### **缺陷模式：遗漏的发布**

需要发布的模块比较多，一些模块遗漏发布，如静态资源、配置等，导致发布后用户使用时发现缺失，影响系统使用

### **缺陷模式：变更操作缺少SOP**

变更操作未按照标准处理流程（SOP）处理，导致故障，比如组件升级标准操作流程

### **缺陷模式：测试&正式环境不统一**

测试环境的配置和正式环境的配置或处理模式不统一，如测试环境下使用了不同的权限校验，正式环境确是用了另外一套权限模式，测试环境验收通过，发布到正式环境导致服务不可用

### 缺陷模式：隐蔽的发布系统或框架变更

日常变更通常只关注自己的代码或者配置的变更，发布系统的变更或者框架的变更往往比较隐蔽，如编译系统默认编译选型的变动、框架代码的升级容易被忽略，历史上也曾因此类变更出现过少量的故障

### 缺陷模式：未被灰度的发布

发布的特性很难当时被验证到，如有周期性的任务或者量级非常少，虽然也是按模块灰度变更，但实际没有被灰度验证到。全量后，等有真正的请求时，等同于全量直接发布，导致故障。规避的方式就是有效的灰度，确实是被验证到了确认没有问题再进一步扩大变更范围，严格遵守“能回滚、可观测、被验证、要平滑”的灰度发布原则。

  

变更问题一般事前整理好checklist，变更过程严格执行灰度发布策略，如果出现故障且能够快速回滚，可以规避很大一部分变更问题，减少影响范围。

# **设计缺陷**

### **缺陷模式：无法水平扩容**

服务过载时，无法通过水平扩容，这类问题常见于脚本、daemon二进制程序、存储，当业务上涨处理不过来时，无法通过扩机器提升系统的处理能力造成任务积压。出现这类的情况只能通过其他的措施，比如手工发版调整进程数或者其他的降级手段缓解，直到恢复。在做系统设计时思考一下，如果遇到了突增请求量或未来 N 倍的业务上涨，能否通过水平扩机器来扛住这些量级，如果不能，堵点在什么地方？对提升可用性会有很大的帮助。

![](https://km.woa.com/asset/00010002241200cb53219f78f7497f02?height=474&width=958)

### **缺陷模式：服务过载而资源不能充分利用**

服务的配置（如进程数）设置不合理，业务量级上涨导致服务过载时，又不能最大限度利用机器资源（也无法触发机器自动扩容等操作）。这类的因素通常是后台模块配置的woker进程或线程数较少，请求量上来之后处理不及时，服务已经过载了，资源利用率也上不去，如CPU利用率不高，就更不会触及到自动扩容的水位了（需要模块事前打开自动扩容的开关）。

### **缺陷模式：不均衡的路由**

历史原因，早期在使用微信后台框架时写业务代码时，接口的RPC路由字段大量使用用户ID （或商户 ID）作为路由的情况。常用的路由算法，如一致性哈希等，相同的ID路由大概率到同一台机器上，当用户大量重试或商户量级不均匀时容易出现路由不均衡的情况，引起单机过载。除此之外，批处理由于排序等原因，也可能会导致请求路由的聚集，一个时间段范围内处理的任务都是同一个路由号段的，分布不均匀。

### **缺陷模式：hardcode问题**

hardcode 了某个特征的业务逻辑，如写死某个商户做怎样的特征处理，或者写死时间某个时间服务不可用。前面的代码缺陷中也提到了 hardcode 的问题，也算是一种设计的问题，这里简单罗列一下，在做系统方案时可以考虑更好的设计方案。

### **缺陷模式：购买资源不足**

有些购买的服务是按 QPS 或者总调用量来收费的，业务刚上线时一般都会购买预估一定时间范围内需要的资源。随着业务的上涨，这里很容易被遗漏，没有及时预警或者预警被忽略了，购买的资源成为瓶颈。

### **缺陷模式：逻辑处理不完备**

异常分支的逻辑处理缺少完备的处理流程，出现异常时无法走到最终解脱的路径，被动等待客诉或人工特征处理。一个系统中完备的处理逻辑比较重要，中间态的数据无法扭转到最终态，需要考虑万一出现了中间态的数据，如何处理。

### **缺陷模式：未按契约实现**

尤其是对接的商户越来越多的时候，很难想象商户会以什么样的方式依赖你的接口返回数据，会有怎样意想不到的设计。如一个不起眼的字段可能能会被商户用来作为分库分表的路由，多一个空格的错误提示文案有可能导致商户的签名验证不过等等，如果这个”非关键字段“又可能被刷新更改，对依赖这个数据的商户有可能会有影响，这也算是契约“隐含”的一部分。另外一种常见的情况就是，最初可能大家一起对方案时，契约是一致的，随着时间推移，需求的迭代，契约就可能会被打破，如新增的未能识别的错误码、新增的字段、突然增加的限流规则等。

### 缺陷模式：设计强依赖当下基础设施

系统设计强依赖基础设施的特性，如业务的生成规则依赖当下机器的部署上下文环境，后续基建的改动或者迁移容易造成未知的后果，同时迁移也会有很大的成本。

### **缺陷模式：兼容性问题**

系统升级，兼容性考虑不足，如不同的客户端、机型的兼容性等。

### **缺陷模式：被遗漏的商户**

系统的改动需要外部商户一起配合改造，部分商户未能及时改造，而又忽略了这些商户，导致商户使用异常

### 缺陷模式：公共地混合部署

这里特别是资源部署公共地，如 DB，KV 等存储、离线数据计算集群。历史原因不少业务存储混合部署在一起，一个业务的使用不当或做活动突发的流量，其他的业务难免会受到影响，存在着隐患。各个不同的业务可用性要求是不一样的，尤其是当可用性要求低的业务和高可用性要求高的业务存储混合部署在一起，可用性低的业务故障不小心影响到可用性要求高的业务。应对措施就是拆分，可用性要求高的业务独立部署。

### **缺陷模式：不合理的限流**

限流的目的是为了保护系统，但不是所有的情况都需要限流，有些场景是不需要做频率限制却做了频率限制，存在过度限制的情况。量级的上涨，影响正常功能的使用。

### **缺陷模式：过小的限流值**

限流的评估通常是以当下的情况进行评估，“当时”是合理的评估，但是业务是变化的。随着时间的推移，请求量级逐步上涨，业务中的这个限制很容易被遗忘，某个时间到达极限之后，影响正常请求，比如在代扣的场景，业务上涨触发限流，很可能会影响正常用户的扣费。

### **缺陷模式：限流的粒度不合理**

1.  限流周期粒度不合理：过大的限流周期，平均下来可能限流值是合理的，请求可能瞬间集中过来打爆服务而又不触发限流
2. 限流范围不合理：如按接口限流，某些商户因活动或者异常导致请求量级突然增大，挤占了限流的空间，影响到其他商户正常使用。

# **IDC故障**

比较典型的两类：  

### **缺陷模式：域名被封禁**

不可抗拒的因素导致域名被封了，虽然大概率可能是被误伤的。最初域名解析通过运营商的LocalDns解析，“单点”难免会有问题，解决方案是多一条路径，如 HttpDns、NewDns 等，LocalDns作为备份，减少被误伤的封禁。另外一个思路是控制影响面，不同业务申请相对独立的域名，即使出现问题也能减少影响到的业务。

**缺陷模式：被挖断的专线**

历史上因为施工或者异常，导致专线异常的情况偶有发生，而且每次出现都容易造成不小的影响。专线的容灾一般需要双路备份，且选择不同的运营商拉取，最好也通过不同的城市接入点接入。冗余备份，多拉一条不同运营商的专线，如果遇到单条被挖断的情况，业务自动熔断，切换到备份的专线，故障恢复后自动切回。

![](https://km.woa.com/asset/000100022501004294606a7e24414802?height=1160&width=962)

# **容量不足**

### **缺陷模式：资源利用配置不当**

进程、协程数配置不合理，进程数较少，机器资源利用率又上不去，也无法触发机器资源过载的自动扩容。

### 缺陷模式：突增的流量

1. cpu间歇性飙升，由于有失败产生，上游的重试又放大了请求量，导致服务压力进一步加大
2. 服务切换，下游服务容量评估不足
3. 任务积压+业务重试，进一步加大了积压，导致资源瓶颈
4. 协层池满，出现了快速拒绝
5. 缺少限流，过载保护及熔断

### 缺陷模式：积压的任务

尤其是有节日效应的业务，陡增的量级导致任务积压，而积压又进一步加剧拖慢任务的正常处理速度。方案设计时可以考虑系统能否有足够的容量处理，一般事前资源评估后通过压测等手段检验系统的处理能力。同时如果有超出预期的流量，能否通过扩容来及时处理任务。

### **缺陷模式：组件边界上限评估不足**  

业务量级超过组件所能承受的最大上限，导致组件过载。不同的组件优势和边界上限情况是不同的，日常做方案时在当前的基础设施的条件下做最优的选择。如业务当前的机型配置下mysql 单表更新 qps 有多少？tablekv 单uin能抗最大 qps 是多少？ttlkv单 key 读写 qps 能到多少？整个集群 qps 能到多少？做到心中有数，方案设计时才能按需选择。

### **缺陷模式："负重"的机器**

除了突发流量外，还有一些日常机器可能负载已经不低了，一直在负重前行，只是没有被关注到，随着自然量级增长或抖动可能就会打破这种平衡，引发容量瓶颈

### 缺陷模式：不均匀的流量

模块的部署用了不同配置的机型，其配置有高有底。这里可能导致的问题有两个：一是不同的机器的模块可能是相同的配置，如进程数，如果兼容了低配机器，高配的机器性能可能无法发挥出来，如果错误配置了高的进程数，低配机器就会负载较高；第二种是不同的机型没有按能力大小分配路由流量，最终导致配置低的成为短板，会首先被击破。

# **操作失误**

人都是不可靠的，人工操作的失误导致一些系统问题：

### **缺陷模式：运营系统使用不当**

1. 人工发布某个配置文件或者通过mis系统修改某个配置项，由于没有按照标准流程规范操作，操作失误也未能被及时发现，发布上了现网，导致服务故障。目前微信支付的配置发布已都要求经过double check确认，且要经过审批流程
2. 系统的易用性较差，容易引发人为操作失误

### **缺陷模式：操作失误**

公告配置失误，导致其他业务被误挂公告；错误配置的券、配置错误的商户等，影响业务正常使用。

### **缺陷模式：操作缺少CheckList**  

涉及多方时，需要明确职责，前置依赖的条件需要各方明确确认后方可操作

### **缺陷模式：误踢的机器**

下线机器错误踢掉了正常的机器，缩容缩掉了所在 set 过多权重的机器等导致不可用或者资源不足，引发故障

### **缺陷模式：压测压垮服务**

和压测策略有关，一次性压测 QPS 较大，资源未隔离，影响到了现网。压测流量过大，到达服务极限或触发限流，出现系统失败或任务积压，影响到了现网的服务，而降级措施又未能及时生效，引发客诉

### **缺陷模式：压测后未及时打扫清理环境**

压测后未及时清理打扫现场，导致服务处理异常。这里的现场，可能是压测的数据、因压测临时配置的开关等。

### **缺陷模式：演习导致故障**

1. 因考虑不周或未知的情况，演习时触发了线上的BUG
2. 存在监控不够导致未能及时发现问题的情况；出现问题，由于突发SOP预案准备不足

# 组件故障

### 缺陷模式：组件故障

业务依赖的组件出现故障，针对依赖的组件，保险的做法针对组件做BCP预案

# **配置问题**

### **缺陷模式：超大的配置文件**

配置过大，往往启动时初始化耗费资源较大；另外一种风险，就是过大的配置文件的配置项存在误修改的风险，尽量避免超大的相互耦合的配置文件。

### **缺陷模式：高风险的全局配置**

“全局”意味着一旦出现问题影响范围可能会比较大，尽量消除全局配置；如果当前不得不维护，严格执行 SOP 的变更流程，多人审批，小心操作。

### **缺陷模式：过期的配置**

主要有三类：一类是设置了资源过期时间，而过期时的处理逻辑又不够完善，随着时间的推移可能就忘记这里还有这个限制，最终导致了异常。第二类是，通过配置控制资源的使用时间，到时间了忘记修改配置，导致服务异常；第三类是在跨部门或者夸BG协作时，需要各自配置多种配置的过期时间，过期时间又未能约定一致，最终导致差错发生。

# 沟通合作

### **缺陷模式：信息未对齐**

1. 跨BG/部门合作，链条比较长，多方信息存在模糊地带。容易出现职责不明确的情况，所以事前明确各个执行路径很重要。
2. 下游变更了协议，如增加或者删除了字段，或者增加了错误码，未通知给上游，上游有强依赖时导致异常。（也算未遵守契约）
3. 下游接口的性能要求未及时沟通，导致资源不够，出现性能瓶颈等等
4. 沟通理解不一致，虽然说的是一个事情，各自的理解不同导致偏差
5. 契约协议描述不清晰，导致不同涉众理解不同
6. 等等...

# **策略问题**  

### **缺陷模式：用户使用习惯的调整**

产品服务使用策略较大调整，导致用户或商户使用不习惯或者产生疑问，引起客诉

### 缺陷模式：白名单问题

这里白名单可能是商户白名单、用户白名单、接口白名单、机器白名单等，可能会被用到多种场景，有的为了控制权限，有的为了安全考虑，只有白名单中的才能触发。而设置白名单机制意味着这里有特征逻辑的存在，需要对这些白名单做特殊的处理，而特殊逻辑的规则一旦无意间打破，就有可能会导致异常。如机器 IP 白名单，扩容时没有添加新增的机器 IP 白名单，就会拒绝服务。

### **缺陷模式：半自动化系统**

组织对外提供价值需要组织内的软件系统和人肉系统共同协作才能完整交付。人肉系统承担了关键的比较重的任务，如未打通的上下游系统，在超过人工能够处理的上限或者节假日效应，容易阻塞业务的完整流程。

# 总结

简单梳理了下微信支付近几年的故障单，从中提取可能导致故障的共性因素，尤其是做业务过程中可能踩坑的地方。侧重分析容易导致这些故障的原因，其应对措施在不同的业务背景和所依赖的基建环境下可能各自有不同最优解。很多时候，只要知道了这里有坑，应对措施反而变得简单，知难行易。具体的风险项也可以有针对性专题研究。这里仅整理了微信支付故障库的一部分，后续有时间再继续深入挖掘。以上结合个人日常做业务过程中一些经验思考进行梳理，有不当之处一起交流学习。